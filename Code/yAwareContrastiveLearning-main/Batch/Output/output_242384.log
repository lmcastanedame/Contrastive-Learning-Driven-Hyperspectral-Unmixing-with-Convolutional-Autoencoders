Starting pretraining...
Loss Function: GeneralizedSupervisedNTXenLoss(temp=0.1, kernel=<lambda>, sigma=5)
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
Epoch [1/200] Training loss = 2.7607 Validation loss = 2.2391	
Epoch [2/200] Training loss = 2.4046 Validation loss = 2.8684	
Epoch [3/200] Training loss = 2.1241 Validation loss = 2.3612	
Epoch [4/200] Training loss = 1.8276 Validation loss = 1.8319	
Epoch [5/200] Training loss = 1.3855 Validation loss = 1.6200	
Epoch [6/200] Training loss = 1.2904 Validation loss = 1.4444	
Epoch [7/200] Training loss = 1.1039 Validation loss = 1.7398	
Epoch [8/200] Training loss = 1.0741 Validation loss = 1.4387	
Epoch [9/200] Training loss = 1.0785 Validation loss = 1.3285	
Epoch [10/200] Training loss = 1.0764 Validation loss = 1.3452	
Epoch [11/200] Training loss = 1.0292 Validation loss = 1.6464	
Epoch [12/200] Training loss = 1.0507 Validation loss = 1.3509	
Epoch [13/200] Training loss = 1.1127 Validation loss = 1.4249	
Epoch [14/200] Training loss = 1.0437 Validation loss = 1.3412	
Epoch [15/200] Training loss = 1.0459 Validation loss = 1.4588	
Epoch [16/200] Training loss = 0.9712 Validation loss = 1.2428	
Epoch [17/200] Training loss = 1.0255 Validation loss = 1.2917	
Epoch [18/200] Training loss = 1.0722 Validation loss = 1.2811	
Epoch [19/200] Training loss = 0.9964 Validation loss = 2.1090	
Epoch [20/200] Training loss = 0.9686 Validation loss = 1.7016	
Epoch [21/200] Training loss = 1.0202 Validation loss = 1.5075	
Epoch [22/200] Training loss = 1.0634 Validation loss = 1.3068	
Epoch [23/200] Training loss = 0.9652 Validation loss = 1.1303	
Epoch [24/200] Training loss = 0.9589 Validation loss = 1.5169	
Epoch [25/200] Training loss = 0.9082 Validation loss = 1.4101	
Epoch [26/200] Training loss = 0.9138 Validation loss = 1.4222	
Epoch [27/200] Training loss = 0.8752 Validation loss = 1.5937	
Epoch [28/200] Training loss = 1.0200 Validation loss = 1.4250	
Epoch [29/200] Training loss = 0.8450 Validation loss = 1.5161	
Epoch [30/200] Training loss = 0.9049 Validation loss = 1.0963	
Epoch [31/200] Training loss = 0.9222 Validation loss = 1.3960	
Epoch [32/200] Training loss = 0.8916 Validation loss = 1.1238	
Epoch [33/200] Training loss = 0.8343 Validation loss = 1.1667	
Epoch [34/200] Training loss = 0.7744 Validation loss = 1.4637	
Epoch [35/200] Training loss = 0.8824 Validation loss = 1.3028	
Epoch [36/200] Training loss = 0.8845 Validation loss = 1.2747	
Epoch [37/200] Training loss = 0.8638 Validation loss = 1.2261	
Epoch [38/200] Training loss = 0.9133 Validation loss = 1.1623	
Epoch [39/200] Training loss = 0.8261 Validation loss = 1.5138	
Epoch [40/200] Training loss = 0.8955 Validation loss = 1.2725	
Epoch [41/200] Training loss = 0.8215 Validation loss = 1.5071	
Epoch [42/200] Training loss = 0.8119 Validation loss = 1.0981	
Epoch [43/200] Training loss = 0.8709 Validation loss = 1.4191	
Epoch [44/200] Training loss = 0.7960 Validation loss = 1.1247	
Epoch [45/200] Training loss = 0.7777 Validation loss = 1.1629	
Epoch [46/200] Training loss = 0.7722 Validation loss = 1.3483	
Epoch [47/200] Training loss = 0.7515 Validation loss = 2.3588	
Epoch [48/200] Training loss = 0.7878 Validation loss = 1.8704	
Epoch [49/200] Training loss = 0.8042 Validation loss = 1.1045	
Epoch [50/200] Training loss = 0.7270 Validation loss = 1.0981	
Epoch [51/200] Training loss = 0.7406 Validation loss = 1.6622	
Epoch [52/200] Training loss = 0.7580 Validation loss = 1.0470	
Epoch [53/200] Training loss = 0.7200 Validation loss = 1.2307	
Epoch [54/200] Training loss = 0.7416 Validation loss = 1.1360	
Epoch [55/200] Training loss = 0.7024 Validation loss = 1.1190	
Epoch [56/200] Training loss = 0.7509 Validation loss = 1.1581	
Epoch [57/200] Training loss = 0.7203 Validation loss = 1.4938	
Epoch [58/200] Training loss = 0.7236 Validation loss = 1.1430	
Epoch [59/200] Training loss = 0.6995 Validation loss = 1.0660	
Epoch [60/200] Training loss = 0.6667 Validation loss = 1.1471	
Epoch [61/200] Training loss = 0.6718 Validation loss = 1.5009	
Epoch [62/200] Training loss = 0.6110 Validation loss = 1.0463	
Epoch [63/200] Training loss = 0.6549 Validation loss = 1.1095	
Epoch [64/200] Training loss = 0.6395 Validation loss = 1.2019	
Epoch [65/200] Training loss = 0.6141 Validation loss = 0.8639	
Epoch [66/200] Training loss = 0.5793 Validation loss = 1.3007	
Epoch [67/200] Training loss = 0.5911 Validation loss = 1.0499	
Epoch [68/200] Training loss = 0.6034 Validation loss = 0.9301	
Epoch [69/200] Training loss = 0.5944 Validation loss = 1.4509	
Epoch [70/200] Training loss = 0.5820 Validation loss = 0.9886	
Epoch [71/200] Training loss = 0.5575 Validation loss = 1.5593	
Epoch [72/200] Training loss = 0.5908 Validation loss = 0.9934	
Epoch [73/200] Training loss = 0.5868 Validation loss = 1.0826	
Epoch [74/200] Training loss = 0.5654 Validation loss = 1.1186	
Epoch [75/200] Training loss = 0.5644 Validation loss = 0.9092	
Epoch [76/200] Training loss = 0.5558 Validation loss = 1.0115	
Epoch [77/200] Training loss = 0.5478 Validation loss = 0.9527	
Epoch [78/200] Training loss = 0.5625 Validation loss = 1.1085	
Epoch [79/200] Training loss = 0.5688 Validation loss = 0.9773	
Epoch [80/200] Training loss = 0.5446 Validation loss = 1.1899	
Epoch [81/200] Training loss = 0.5193 Validation loss = 1.0029	
Epoch [82/200] Training loss = 0.5225 Validation loss = 1.7986	
Epoch [83/200] Training loss = 0.4918 Validation loss = 0.9437	
Epoch [84/200] Training loss = 0.5044 Validation loss = 0.8321	
Epoch [85/200] Training loss = 0.5141 Validation loss = 1.6824	
Epoch [86/200] Training loss = 0.4607 Validation loss = 0.8810	
Epoch [87/200] Training loss = 0.5279 Validation loss = 0.8535	
Epoch [88/200] Training loss = 0.5325 Validation loss = 0.9312	
Epoch [89/200] Training loss = 0.4901 Validation loss = 1.0516	
Epoch [90/200] Training loss = 0.5384 Validation loss = 1.0187	
Epoch [91/200] Training loss = 0.5355 Validation loss = 0.9972	
Epoch [92/200] Training loss = 0.5330 Validation loss = 0.8792	
Epoch [93/200] Training loss = 0.5273 Validation loss = 0.9674	
Epoch [94/200] Training loss = 0.4766 Validation loss = 1.2823	
Epoch [95/200] Training loss = 0.5604 Validation loss = 0.7717	
Epoch [96/200] Training loss = 0.4628 Validation loss = 0.9558	
Epoch [97/200] Training loss = 0.5047 Validation loss = 0.7623	
Epoch [98/200] Training loss = 0.4891 Validation loss = 0.8130	
Epoch [99/200] Training loss = 0.4788 Validation loss = 1.3950	
Epoch [100/200] Training loss = 0.4953 Validation loss = 1.2884	
Epoch [101/200] Training loss = 0.4584 Validation loss = 0.8059	
Epoch [102/200] Training loss = 0.5109 Validation loss = 0.9370	
Epoch [103/200] Training loss = 0.4717 Validation loss = 0.8758	
Epoch [104/200] Training loss = 0.4710 Validation loss = 1.0652	
Epoch [105/200] Training loss = 0.4771 Validation loss = 0.8113	
Epoch [106/200] Training loss = 0.5133 Validation loss = 1.0297	
Epoch [107/200] Training loss = 0.4416 Validation loss = 0.8812	
Epoch [108/200] Training loss = 0.4650 Validation loss = 0.8761	
Epoch [109/200] Training loss = 0.4625 Validation loss = 0.7141	
Epoch [110/200] Training loss = 0.4740 Validation loss = 0.9404	
Epoch [111/200] Training loss = 0.4856 Validation loss = 0.7928	
Epoch [112/200] Training loss = 0.4989 Validation loss = 0.9156	
Epoch [113/200] Training loss = 0.5124 Validation loss = 1.2646	
Epoch [114/200] Training loss = 0.4292 Validation loss = 0.7186	
Epoch [115/200] Training loss = 0.4629 Validation loss = 0.7803	
Epoch [116/200] Training loss = 0.4569 Validation loss = 0.7675	
Epoch [117/200] Training loss = 0.5091 Validation loss = 0.8062	
Epoch [118/200] Training loss = 0.4814 Validation loss = 0.7814	
Epoch [119/200] Training loss = 0.4472 Validation loss = 0.9915	
Epoch [120/200] Training loss = 0.4605 Validation loss = 1.9716	
Epoch [121/200] Training loss = 0.4369 Validation loss = 1.1451	
Epoch [122/200] Training loss = 0.4683 Validation loss = 0.7935	
Epoch [123/200] Training loss = 0.4903 Validation loss = 1.0349	
Epoch [124/200] Training loss = 0.4615 Validation loss = 1.1254	
Epoch [125/200] Training loss = 0.4785 Validation loss = 0.8228	
Epoch [126/200] Training loss = 0.4451 Validation loss = 1.3717	
Epoch [127/200] Training loss = 0.4773 Validation loss = 0.8061	
Epoch [128/200] Training loss = 0.4548 Validation loss = 1.9131	
Epoch [129/200] Training loss = 0.4710 Validation loss = 0.8946	
Epoch [130/200] Training loss = 0.4370 Validation loss = 1.2084	
Epoch [131/200] Training loss = 0.4282 Validation loss = 0.8324	
Epoch [132/200] Training loss = 0.4362 Validation loss = 0.9006	
Epoch [133/200] Training loss = 0.4630 Validation loss = 0.8423	
Epoch [134/200] Training loss = 0.4553 Validation loss = 0.9756	
Epoch [135/200] Training loss = 0.4361 Validation loss = 0.8340	
Epoch [136/200] Training loss = 0.4564 Validation loss = 0.8795	
Epoch [137/200] Training loss = 0.4092 Validation loss = 1.0669	
Epoch [138/200] Training loss = 0.4685 Validation loss = 0.8423	
Epoch [139/200] Training loss = 0.4213 Validation loss = 0.9651	
Epoch [140/200] Training loss = 0.4051 Validation loss = 0.8776	
Epoch [141/200] Training loss = 0.4297 Validation loss = 1.1727	
Epoch [142/200] Training loss = 0.4603 Validation loss = 1.2285	
Epoch [143/200] Training loss = 0.4607 Validation loss = 0.8507	
Epoch [144/200] Training loss = 0.4472 Validation loss = 0.8047	
Epoch [145/200] Training loss = 0.3829 Validation loss = 0.8151	
Epoch [146/200] Training loss = 0.4428 Validation loss = 0.8149	
Epoch [147/200] Training loss = 0.4280 Validation loss = 1.6873	
Epoch [148/200] Training loss = 0.4601 Validation loss = 0.8119	
Epoch [149/200] Training loss = 0.4426 Validation loss = 2.0608	
Epoch [150/200] Training loss = 0.4493 Validation loss = 1.0370	
Epoch [151/200] Training loss = 0.4066 Validation loss = 1.6599	
Epoch [152/200] Training loss = 0.4409 Validation loss = 0.9428	
Epoch [153/200] Training loss = 0.4424 Validation loss = 1.0112	
Epoch [154/200] Training loss = 0.4159 Validation loss = 0.6848	
Epoch [155/200] Training loss = 0.4535 Validation loss = 0.8264	
Epoch [156/200] Training loss = 0.4270 Validation loss = 0.9226	
Epoch [157/200] Training loss = 0.4104 Validation loss = 1.0714	
Epoch [158/200] Training loss = 0.4222 Validation loss = 0.9974	
Epoch [159/200] Training loss = 0.4176 Validation loss = 0.8476	
Epoch [160/200] Training loss = 0.4328 Validation loss = 0.8347	
Epoch [161/200] Training loss = 0.4183 Validation loss = 0.8720	
Epoch [162/200] Training loss = 0.4461 Validation loss = 0.8360	
Epoch [163/200] Training loss = 0.4466 Validation loss = 1.0341	
Epoch [164/200] Training loss = 0.4262 Validation loss = 1.4715	
Epoch [165/200] Training loss = 0.4246 Validation loss = 2.4020	
Epoch [166/200] Training loss = 0.4063 Validation loss = 0.7740	
Epoch [167/200] Training loss = 0.4195 Validation loss = 0.8021	
Epoch [168/200] Training loss = 0.4730 Validation loss = 0.9007	
Epoch [169/200] Training loss = 0.4314 Validation loss = 0.9867	
Epoch [170/200] Training loss = 0.3810 Validation loss = 0.7859	
Epoch [171/200] Training loss = 0.3703 Validation loss = 1.7625	
Epoch [172/200] Training loss = 0.4141 Validation loss = 1.2685	
Epoch [173/200] Training loss = 0.4363 Validation loss = 0.7816	
Epoch [174/200] Training loss = 0.3548 Validation loss = 0.9026	
Epoch [175/200] Training loss = 0.3785 Validation loss = 1.0380	
Epoch [176/200] Training loss = 0.4344 Validation loss = 1.3165	
Epoch [177/200] Training loss = 0.3954 Validation loss = 1.7537	
Epoch [178/200] Training loss = 0.4066 Validation loss = 0.9011	
Epoch [179/200] Training loss = 0.3851 Validation loss = 1.1496	
Epoch [180/200] Training loss = 0.4285 Validation loss = 1.1666	
Epoch [181/200] Training loss = 0.3984 Validation loss = 0.7653	
Epoch [182/200] Training loss = 0.4115 Validation loss = 0.9601	
Epoch [183/200] Training loss = 0.4555 Validation loss = 0.7566	
Epoch [184/200] Training loss = 0.4580 Validation loss = 0.8842	
Epoch [185/200] Training loss = 0.3848 Validation loss = 0.8418	
Epoch [186/200] Training loss = 0.4035 Validation loss = 1.0843	
Epoch [187/200] Training loss = 0.4408 Validation loss = 0.9109	
Epoch [188/200] Training loss = 0.4367 Validation loss = 1.2346	
Epoch [189/200] Training loss = 0.4020 Validation loss = 1.3549	
Epoch [190/200] Training loss = 0.4037 Validation loss = 0.9155	
Epoch [191/200] Training loss = 0.3956 Validation loss = 0.8693	
Epoch [192/200] Training loss = 0.4059 Validation loss = 1.7559	
Epoch [193/200] Training loss = 0.4301 Validation loss = 1.6490	
Epoch [194/200] Training loss = 0.4007 Validation loss = 1.1073	
Epoch [195/200] Training loss = 0.3781 Validation loss = 1.0711	
Epoch [196/200] Training loss = 0.3763 Validation loss = 1.1639	
Epoch [197/200] Training loss = 0.4223 Validation loss = 1.4868	
Epoch [198/200] Training loss = 0.3750 Validation loss = 1.0998	
Epoch [199/200] Training loss = 0.4102 Validation loss = 1.0218	
Epoch [200/200] Training loss = 0.4482 Validation loss = 1.0365	
