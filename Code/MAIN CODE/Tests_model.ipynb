{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTXEN LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from training_dataset_functions import train_epoch, load_HSI, HSI_Dataset\n",
    "from autoencoder import Autoencoder\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "\n",
    "def order_endmembers(endmembers, endmembersGT):\n",
    "    num_endmembers = endmembers.shape[0]\n",
    "    dict = {}\n",
    "    sad_mat = np.ones((num_endmembers, num_endmembers))\n",
    "    for i in range(num_endmembers):\n",
    "        endmembers[i, :] = endmembers[i, :] / endmembers[i, :].max()\n",
    "        endmembersGT[i, :] = endmembersGT[i, :] / endmembersGT[i, :].max()\n",
    "    for i in range(num_endmembers):\n",
    "        for j in range(num_endmembers):\n",
    "            sad_mat[i, j] = numpy_SAD(endmembers[i, :], endmembersGT[j, :])\n",
    "    rows = 0\n",
    "    while rows < num_endmembers:\n",
    "        minimum = sad_mat.min()\n",
    "        index_arr = np.where(sad_mat == minimum)\n",
    "        if len(index_arr) < 2:\n",
    "            break\n",
    "        index = (index_arr[0][0], index_arr[1][0])\n",
    "        if index[0] in dict.keys():\n",
    "            sad_mat[index[0], index[1]] = 100\n",
    "        elif index[1] in dict.values():\n",
    "            sad_mat[index[0], index[1]] = 100\n",
    "        else:\n",
    "            dict[index[0]] = index[1]\n",
    "            sad_mat[index[0], index[1]] = 100\n",
    "            rows += 1\n",
    "    ASAM = 0\n",
    "    num = 0\n",
    "    for i in range(num_endmembers):\n",
    "        if np.var(endmembersGT[dict[i]]) > 0:\n",
    "            ASAM = ASAM + numpy_SAD(endmembers[i, :], endmembersGT[dict[i]])\n",
    "            num += 1\n",
    "\n",
    "    return dict, ASAM / float(num)\n",
    "\n",
    "def numpy_SAD(y_true, y_pred):\n",
    "    return np.arccos(np.clip(y_pred.dot(y_true) / (np.linalg.norm(y_true) * np.linalg.norm(y_pred)), -1.0, 1.0))\n",
    "\n",
    "def plot_endmembers(true_endmembers, predicted_endmembers):\n",
    "    \"\"\"\n",
    "    Plots the true and predicted endmembers in separate subplots.\n",
    "    Each subplot corresponds to a single endmember.\n",
    "    \"\"\"\n",
    "    # Normalize each set independently\n",
    "    true_endmembers = true_endmembers / np.max(true_endmembers, axis=1, keepdims=True)\n",
    "    predicted_endmembers = predicted_endmembers / np.max(predicted_endmembers, axis=1, keepdims=True)\n",
    "    \n",
    "    num_endmembers = true_endmembers.shape[0]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_endmembers, 1, figsize=(8, 2 * num_endmembers), sharex=True)\n",
    "\n",
    "    # Generate distinct colors for each endmember\n",
    "    colors = sns.color_palette(\"husl\", num_endmembers)  \n",
    "\n",
    "    for i in range(num_endmembers):\n",
    "        ax = axes[i] if num_endmembers > 1 else axes  # Handle single subplot case\n",
    "        ax.plot(true_endmembers[i, :], label=f'True {i+1}', linestyle='-', color=colors[i])\n",
    "        ax.plot(predicted_endmembers[i, :], label=f'Predicted {i+1}', linestyle='--', color=colors[i])\n",
    "        ax.legend()\n",
    "        ax.set_ylabel(\"Reflectance\")\n",
    "        ax.set_title(f\"Endmember {i+1}\")\n",
    "\n",
    "    plt.xlabel(\"Bands\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# List of datasets\n",
    "datasets = {\n",
    "    'Samson': {'bands': 156, 'endmembers': 3},\n",
    "    'Urban4': {'bands': 162, 'endmembers': 4},\n",
    "    # 'Urban5': {'bands': 162, 'endmembers': 5},\n",
    "    # 'Urban6': {'bands': 162, 'endmembers': 6},\n",
    "    # 'Cuprite_fixed': {'bands': 188, 'endmembers': 12},\n",
    "    # 'JasperRidge': {'bands': 198, 'endmembers': 4},\n",
    "}\n",
    "\n",
    "# Set best hyperparameters\n",
    "best_hyperparams = {\n",
    "    'temperature': 0.01,\n",
    "    'lambda_recon': 0.9,\n",
    "    'patch_size': 8,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for dataset_name, dataset_params in datasets.items():\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    # Clear GPU memory to avoid issues when switching datasets\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set seed globally at the beginning\n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Ensure no randomness in convolutions\n",
    "    torch.use_deterministic_algorithms(True)  # Force deterministic operations\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # DataLoader with seed\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)  # Ensure Python's hash functions are deterministic\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # Ensure deterministic behavior for cuBLAS\n",
    "    \n",
    "    # Fixed parameters\n",
    "    params = {\n",
    "        'n_bands': dataset_params['bands'],\n",
    "        'e_filters': 48,\n",
    "        'e_size': 3,\n",
    "        'd_filters': dataset_params['bands'],\n",
    "        'd_size': 13,\n",
    "        'num_endmembers': dataset_params['endmembers'],\n",
    "        'scale': 3,\n",
    "        'lr': 0.003,\n",
    "        'weight_decay': 1e-6,\n",
    "        'alpha_range': (0.8, 1.2), \n",
    "        'num_patches': 250\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load dataset\n",
    "    data, GT, S_GT = load_HSI(f'./Datasets/{dataset_name}.mat')\n",
    "    \n",
    "    # Prepare dataset\n",
    "    hsi_dataset = HSI_Dataset(data, best_hyperparams['patch_size'], params['num_patches'], alpha_range=params['alpha_range'], random_state=42)\n",
    "    data_loader = DataLoader(\n",
    "                hsi_dataset,\n",
    "                batch_size=best_hyperparams['batch_size'],\n",
    "                shuffle=True,  # Keep shuffle but ensure consistent results\n",
    "                collate_fn=lambda x: (\n",
    "                    torch.stack([item[0] for item in x]),  # Patches\n",
    "                    torch.tensor([item[1] for item in x])  # Alphas\n",
    "                ),\n",
    "                num_workers=0,  # Setting workers to 0 ensures full reproducibility (multi-threading can introduce randomness)\n",
    "                worker_init_fn=lambda worker_id: np.random.seed(seed_value + worker_id),  # Ensure workers get the same seed\n",
    "                generator=g\n",
    "            )\n",
    "    \n",
    "    model = Autoencoder({**params, 'patch_size': best_hyperparams['patch_size']}).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    \n",
    "    # Extract labels for contrastive learning\n",
    "    patches = extract_patches_2d(S_GT, (best_hyperparams['patch_size'], best_hyperparams['patch_size']), max_patches=params['num_patches'], random_state=42)\n",
    "    labels = patches.mean(axis=(1, 2))\n",
    "    labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    for epoch in range(best_hyperparams['epochs']):\n",
    "        total_loss, contrastive_loss, recon_loss = train_epoch(\n",
    "            model, data_loader, optimizer, labels, device,\n",
    "            temperature=best_hyperparams['temperature'], lambda_recon=best_hyperparams['lambda_recon'],\n",
    "            loss_type='ntxen', kernel='rbf', sigma_kernel=0.1\n",
    "        )\n",
    "        print(f\"Epoch {epoch + 1}/{best_hyperparams['epochs']} - Total Loss: {total_loss:.4f}, Contrastive Loss: {contrastive_loss:.4f}, Recon Loss: {recon_loss:.4f}\", flush=True)\n",
    "    \n",
    "    # Extract non-negative endmembers from the decoder\n",
    "    endmembers = F.relu(model.decoder.output_layer.weight_raw).detach().cpu().numpy()\n",
    "\n",
    "    if endmembers.shape[2] > 1:\n",
    "        endmembers = np.squeeze(endmembers).mean(axis=2).mean(axis=2)\n",
    "    else:\n",
    "        endmembers = np.squeeze(endmembers)\n",
    "\n",
    "    predicted_endmembers = endmembers.T\n",
    "    true_endmembers = GT\n",
    "\n",
    "    # Normalize endmembers\n",
    "    for m in range(true_endmembers.shape[0]):\n",
    "        predicted_endmembers[m, :] = predicted_endmembers[m, :] / predicted_endmembers[m, :].max()\n",
    "        true_endmembers[m, :] = true_endmembers[m, :] / true_endmembers[m, :].max()\n",
    "\n",
    "    # Calculate SAD values\n",
    "    order_dict, mean_sad = order_endmembers(true_endmembers, predicted_endmembers)\n",
    "    reordered_predicted_endmembers = predicted_endmembers[[order_dict[k] for k in sorted(order_dict.keys())]]\n",
    "\n",
    "    run_sad_values = [numpy_SAD(reordered_predicted_endmembers[j, :], true_endmembers[j, :]) for j in range(true_endmembers.shape[0])]\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Mean SAD: {np.mean(run_sad_values)}\")\n",
    "    print(run_sad_values)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_endmembers(true_endmembers, reordered_predicted_endmembers)\n",
    "    \n",
    "    del model, optimizer, data_loader, hsi_dataset, data, GT, S_GT, patches, labels\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERALIZED LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from training_dataset_functions import train_epoch, load_HSI, HSI_Dataset\n",
    "from autoencoder import Autoencoder\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "\n",
    "def order_endmembers(endmembers, endmembersGT):\n",
    "    num_endmembers = endmembers.shape[0]\n",
    "    dict = {}\n",
    "    sad_mat = np.ones((num_endmembers, num_endmembers))\n",
    "    for i in range(num_endmembers):\n",
    "        endmembers[i, :] = endmembers[i, :] / endmembers[i, :].max()\n",
    "        endmembersGT[i, :] = endmembersGT[i, :] / endmembersGT[i, :].max()\n",
    "    for i in range(num_endmembers):\n",
    "        for j in range(num_endmembers):\n",
    "            sad_mat[i, j] = numpy_SAD(endmembers[i, :], endmembersGT[j, :])\n",
    "    rows = 0\n",
    "    while rows < num_endmembers:\n",
    "        minimum = sad_mat.min()\n",
    "        index_arr = np.where(sad_mat == minimum)\n",
    "        if len(index_arr) < 2:\n",
    "            break\n",
    "        index = (index_arr[0][0], index_arr[1][0])\n",
    "        if index[0] in dict.keys():\n",
    "            sad_mat[index[0], index[1]] = 100\n",
    "        elif index[1] in dict.values():\n",
    "            sad_mat[index[0], index[1]] = 100\n",
    "        else:\n",
    "            dict[index[0]] = index[1]\n",
    "            sad_mat[index[0], index[1]] = 100\n",
    "            rows += 1\n",
    "    ASAM = 0\n",
    "    num = 0\n",
    "    for i in range(num_endmembers):\n",
    "        if np.var(endmembersGT[dict[i]]) > 0:\n",
    "            ASAM = ASAM + numpy_SAD(endmembers[i, :], endmembersGT[dict[i]])\n",
    "            num += 1\n",
    "\n",
    "    return dict, ASAM / float(num)\n",
    "\n",
    "def numpy_SAD(y_true, y_pred):\n",
    "    return np.arccos(np.clip(y_pred.dot(y_true) / (np.linalg.norm(y_true) * np.linalg.norm(y_pred)), -1.0, 1.0))\n",
    "\n",
    "def plot_endmembers(true_endmembers, predicted_endmembers):\n",
    "    \"\"\"\n",
    "    Plots the true and predicted endmembers in separate subplots.\n",
    "    Each subplot corresponds to a single endmember.\n",
    "    \"\"\"\n",
    "    # Normalize each set independently\n",
    "    true_endmembers = true_endmembers / np.max(true_endmembers, axis=1, keepdims=True)\n",
    "    predicted_endmembers = predicted_endmembers / np.max(predicted_endmembers, axis=1, keepdims=True)\n",
    "    \n",
    "    num_endmembers = true_endmembers.shape[0]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_endmembers, 1, figsize=(8, 2 * num_endmembers), sharex=True)\n",
    "\n",
    "    # Generate distinct colors for each endmember\n",
    "    colors = sns.color_palette(\"husl\", num_endmembers)  \n",
    "\n",
    "    for i in range(num_endmembers):\n",
    "        ax = axes[i] if num_endmembers > 1 else axes  # Handle single subplot case\n",
    "        ax.plot(true_endmembers[i, :], label=f'True {i+1}', linestyle='-', color=colors[i])\n",
    "        ax.plot(predicted_endmembers[i, :], label=f'Predicted {i+1}', linestyle='--', color=colors[i])\n",
    "        ax.legend()\n",
    "        ax.set_ylabel(\"Reflectance\")\n",
    "        ax.set_title(f\"Endmember {i+1}\")\n",
    "\n",
    "    plt.xlabel(\"Bands\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Function to plot losses\n",
    "def plot_losses(contrastive_losses, recon_losses, total_losses, dataset_name):\n",
    "    epochs = range(1, len(contrastive_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Contrastive Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, contrastive_losses, label=\"Contrastive Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Contrastive Loss vs. Epochs ({dataset_name})\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Reconstruction Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, recon_losses, label=\"Reconstruction Loss\", color=\"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Reconstruction Loss vs. Epochs ({dataset_name})\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# List of datasets\n",
    "datasets = {\n",
    "    'Samson': {'bands': 156, 'endmembers': 3},\n",
    "    'Urban4': {'bands': 162, 'endmembers': 4},\n",
    "    # 'Urban5': {'bands': 162, 'endmembers': 5},\n",
    "    # 'Urban6': {'bands': 162, 'endmembers': 6},\n",
    "    # 'Cuprite_fixed': {'bands': 188, 'endmembers': 12},\n",
    "    'JasperRidge': {'bands': 198, 'endmembers': 4},\n",
    "}\n",
    "\n",
    "# Set best hyperparameters\n",
    "best_hyperparams = {\n",
    "    'temperature': 0.1,\n",
    "    'lambda_recon': 0.5,\n",
    "    'patch_size': 32,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for dataset_name, dataset_params in datasets.items():\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    # Clear GPU memory to avoid issues when switching datasets\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set seed globally at the beginning\n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Ensure no randomness in convolutions\n",
    "    torch.use_deterministic_algorithms(True)  # Force deterministic operations\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # DataLoader with seed\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)  # Ensure Python's hash functions are deterministic\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # Ensure deterministic behavior for cuBLAS\n",
    "    \n",
    "    # Fixed parameters\n",
    "    params = {\n",
    "        'n_bands': dataset_params['bands'],\n",
    "        'e_filters': 48,\n",
    "        'e_size': 3,\n",
    "        'd_filters': dataset_params['bands'],\n",
    "        'd_size': 13,\n",
    "        'num_endmembers': dataset_params['endmembers'],\n",
    "        'scale': 3,\n",
    "        'lr': 0.003,\n",
    "        'weight_decay': 1e-6,\n",
    "        'alpha_range': (0.8, 1.2), \n",
    "        'num_patches': 250\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load dataset\n",
    "    data, GT, S_GT = load_HSI(f'./Datasets/{dataset_name}.mat')\n",
    "    \n",
    "    # Prepare dataset\n",
    "    hsi_dataset = HSI_Dataset(data, best_hyperparams['patch_size'], params['num_patches'], alpha_range=params['alpha_range'], random_state=42)\n",
    "    data_loader = DataLoader(\n",
    "                    hsi_dataset,\n",
    "                    batch_size=best_hyperparams['batch_size'],\n",
    "                    shuffle=True,  # Keep shuffle but ensure consistent results\n",
    "                    collate_fn=lambda x: (\n",
    "                        torch.stack([item[0] for item in x]),  # Patches\n",
    "                        torch.tensor([item[1] for item in x])  # Alphas\n",
    "                    ),\n",
    "                    num_workers=0,  # Setting workers to 0 ensures full reproducibility (multi-threading can introduce randomness)\n",
    "                    worker_init_fn=lambda worker_id: np.random.seed(seed_value + worker_id),  # Ensure workers get the same seed\n",
    "                    generator=g\n",
    "                )\n",
    "    \n",
    "    model = Autoencoder({**params, 'patch_size': best_hyperparams['patch_size']}).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    \n",
    "    # labels = None\n",
    "    \n",
    "    patches = extract_patches_2d(S_GT, (best_hyperparams['patch_size'], best_hyperparams['patch_size']), max_patches=params['num_patches'], random_state=42)\n",
    "    labels = patches.mean(axis=(1, 2))\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Store losses for visualization\n",
    "    contrastive_losses = []\n",
    "    recon_losses = []\n",
    "    total_losses = []\n",
    "    \n",
    "    # Train model\n",
    "    for epoch in range(best_hyperparams['epochs']):\n",
    "        total_loss, contrastive_loss, recon_loss = train_epoch(\n",
    "            model, data_loader, optimizer, labels, device,\n",
    "            temperature=best_hyperparams['temperature'], lambda_recon=best_hyperparams['lambda_recon'],\n",
    "            loss_type='generalized', kernel='rbf', sigma_kernel=1\n",
    "        )\n",
    "        \n",
    "        # Store losses for plotting\n",
    "        contrastive_losses.append(contrastive_loss)\n",
    "        recon_losses.append(recon_loss)\n",
    "        total_losses.append(total_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{best_hyperparams['epochs']} - Total Loss: {total_loss:.4f}, Contrastive Loss: {contrastive_loss:.4f}, Recon Loss: {recon_loss:.4f}\", flush=True)\n",
    "    \n",
    "    # Plot losses\n",
    "    plot_losses(contrastive_losses, recon_losses, total_losses, dataset_name)\n",
    "    \n",
    "    # Extract endmembers\n",
    "    endmembers = F.relu(model.decoder.output_layer.weight_raw).detach().cpu().numpy()\n",
    "    \n",
    "    if endmembers.shape[2] > 1:\n",
    "        endmembers = np.squeeze(endmembers).mean(axis=2).mean(axis=2)\n",
    "    else:\n",
    "        endmembers = np.squeeze(endmembers)\n",
    "    \n",
    "    predicted_endmembers = endmembers.T\n",
    "    true_endmembers = GT\n",
    "    \n",
    "    # Calculate SAD values\n",
    "    order_dict, _ = order_endmembers(true_endmembers, predicted_endmembers)\n",
    "    reordered_predicted_endmembers = predicted_endmembers[[order_dict[k] for k in sorted(order_dict.keys())]]\n",
    "    \n",
    "    run_sad_values = [numpy_SAD(reordered_predicted_endmembers[j, :], true_endmembers[j, :]) for j in range(true_endmembers.shape[0])]\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Mean SAD: {np.mean(run_sad_values)}\")\n",
    "    print(run_sad_values)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_endmembers(true_endmembers, reordered_predicted_endmembers)\n",
    "    \n",
    "    del model, optimizer, data_loader, hsi_dataset, data, GT, S_GT, patches, labels\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNNAEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
